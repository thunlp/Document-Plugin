[train] #train parameters
epoch = 16
batch_size = 16

shuffle = True

reader_num = 1

optimizer = AdamW
learning_rate = 2e-6
weight_decay = 1e-5

multilabel = False
warmup_steps=2000
training_steps=50000
max_grad_norm=1.0

ignore_no_grad=False

save_step=-1

[eval] #eval parameters

shuffle = False
reader_num = 1

[distributed]
use = True
backend = nccl

[data] #data parameters


[model] #model parameters
pretrained_model_path = checkpoint/PLMs/t5-large

adapter_path = None

[output] #output parameters
output_time = 10
test_time = 1
output_grad_step=200

model_path = checkpoint

output_function = squad